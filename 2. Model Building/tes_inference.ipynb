{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbabd5aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 13:54:00.576774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 13:54:00.620907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 13:54:00.621622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 13:54:00.622898: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-07 13:54:00.623339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 13:54:00.624056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 13:54:00.624758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 13:54:01.145861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 13:54:01.146540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 13:54:01.147361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 13:54:01.147935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13823 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-06-07 13:54:06.859395: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 38988800 exceeds 10% of free system memory.\n",
      "2022-06-07 13:54:06.859507: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 77977600 exceeds 10% of free system memory.\n",
      "2022-06-07 13:54:06.859706: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 79891456 exceeds 10% of free system memory.\n",
      "2022-06-07 13:54:06.859907: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 77977600 exceeds 10% of free system memory.\n",
      "2022-06-07 13:54:06.860093: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 79891456 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('baseline 2 saved/baseline_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bf8e1",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6774bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text to get the vocab count \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "def import_tokenizer(file_path):\n",
    "    with open(file_path, 'rb') as token:\n",
    "        tokenizer = pickle.load(token)\n",
    "    return tokenizer\n",
    "\n",
    "x_tokenizer = import_tokenizer('baseline 2 saved/x_tokenizer.pickle')\n",
    "y_tokenizer = import_tokenizer('baseline 2 saved/y_tokenizer.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535a82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Dictionary for Source Vocabulary\n",
    "reverse_target_word_index=y_tokenizer.index_word \n",
    "reverse_source_word_index=x_tokenizer.index_word \n",
    "target_word_index=y_tokenizer.word_index\n",
    "y_vocab = y_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea0b87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8c7e8",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd1fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, \\\n",
    "    Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "latent_dim = 256\n",
    "embedding_dim = 256\n",
    "max_len_content = 500\n",
    "max_len_summary = 62\n",
    "\n",
    "\n",
    "#Inference/Validation Phase\n",
    "#Encoding the input sequence to get the feature vector\n",
    "encoder_inputs = model.input[0]\n",
    "encoder_outputs, state_h, state_c = model.layers[6].output\n",
    "\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "#Decoder setup\n",
    "#These tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_len_content,latent_dim))\n",
    "\n",
    "#Getting the embeddings of the decoder sequence\n",
    "dec_emb2= model.layers[5].output\n",
    "\n",
    "#Setting the initial states to the states from the previous time step for better prediction\n",
    "decoder_lstm = model.layers[7]\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#Attention inference\n",
    "attn_layer = model.layers[8]\n",
    "\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "#decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "decoder_inf_concat = model.layers[9]([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "#Adding Dense softmax layer to generate proability distribution over the target vocabulary\n",
    "decoder_dense = model.layers[10]\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "#Final Decoder model\n",
    "decoder_inputs = model.input[1]\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1da4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function defining the implementation of inference process\n",
    "import numpy as np\n",
    "def decode_sequence(input_seq):\n",
    "    #Encoding the input as state vectors\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    #Generating empty target sequence of length 1\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    #Populating the first word of target sequence with the start word\n",
    "    target_seq[0, 0] = target_word_index['START']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        #Sampling a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if sampled_token_index == 0:\n",
    "            continue\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if sampled_token != 'END':\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #Exit condition: either hit max length or find stop word\n",
    "        if (sampled_token == 'END' or len(decoded_sentence.split()) >= (max_len_summary-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        #Updating the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        #Updating internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96fae6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to convert an integer sequence to a word sequence for summary as well as reviews \n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['START']) and i!=target_word_index['END']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a9af43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk import flatten\n",
    "\n",
    "def inference(text, padding_type='post', truncation_type='post'):\n",
    "    text = [text]\n",
    "    text_infer = x_tokenizer.texts_to_sequences(text)\n",
    "    text_infer = pad_sequences(text_infer, maxlen=max_len_content, padding=padding_type, truncating=truncation_type)\n",
    "    text_infer = decode_sequence(text_infer.reshape(1,max_len_content))\n",
    "    return text_infer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32057099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(text):\n",
    "    prediction = inference(text)\n",
    "    print('Original Text\\n')\n",
    "    print(text)\n",
    "    print('Predicted Summary\\n')\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f0701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
